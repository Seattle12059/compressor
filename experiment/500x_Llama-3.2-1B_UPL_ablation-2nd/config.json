{
    "pretrain_training_config": {
        "model_id": "meta-llama/Llama-3.2-1B",
        "chunk_size": 510,
        "total_batch_size": 16,
        "batch_size_per_device": 1,
        "device_count": 4,
        "gradient_accumulation_steps": 4,
        "learning_rate": 0.0001,
        "max_grad_norm": 2.0,
        "log_step": 100,
        "valid_step": 100000000,
        "save_step": 10000
    },
    "pretrain_task_config": {
        "is_pretrain":true,
        "is_sft":false,
        "chunk_size": 510,
        "mem_size": 102,
        "compress_ratio": 5,
        "task_type": "Compress",
        "use_pe": true,
        "use_ae_loss": true,
        "use_lm_loss": true
    },
    "sft_training_config": {
        "model_id": "meta-llama/Llama-3.2-1B",
        "chunk_size": 510,
        "total_batch_size": 16,
        "batch_size_per_device": 1,
        "device_count": 4,
        "gradient_accumulation_steps": 4,
        "learning_rate": 0.00005,
        "max_grad_norm": 2.0,
        "log_step": 100,
        "valid_step": 100000000,
        "save_step": 100000000
    },
    "sft_task_config": {
        "is_pretrain":false,
        "is_sft":true,
        "chunk_size": 510,
        "mem_size": 102,
        "compress_ratio": 5,
        "task_type": "Compress",
        "use_pe": true,
        "use_ae_loss": false,
        "use_lm_loss": true
    },
    "data_config": {
        "dataset_repo": "DKYoon/SlimPajama-6B",
        "samples_num": 320000,
        "min_len": 510,
        "max_len": 2040,
        "model_id": "meta-llama/Llama-3.2-1B",
        "instruction_dataset_repo": "bigData/mrqa-workshop_mrqa"
    }
}