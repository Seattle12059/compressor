{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-14T13:07:48.484500Z",
     "start_time": "2025-11-14T13:07:45.308576Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 如果 modeling.py 在 new_code/pretrain 目录下，这个脚本就放在同一层的 new_code 里，\n",
    "# 然后这样 import：\n",
    "from pretrain.modeling import load_model_with_adapter\n",
    "\n",
    "\n",
    "def build_task_config(chunk_size=500, mem_size=1, compress_ratio=500):\n",
    "    \"\"\"\n",
    "    这里的配置要和你训练时保持一致：\n",
    "    chunk_size=500, mem_size=1, compress_ratio=500\n",
    "    \"\"\"\n",
    "    task_config = {\n",
    "        \"task_type\": \"Compress\",\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"mem_size\": mem_size,\n",
    "        \"compress_ratio\": compress_ratio,\n",
    "        # 下面这些 flag 主要是训练时用的，推理时没太大关系，但保持合理即可\n",
    "        \"is_pretrain\": False,\n",
    "        \"is_sft\": True,        # 现在是 SFT 后的 QA 模型\n",
    "        \"use_pe\": True,\n",
    "        \"use_ae_loss\": True,   # 只影响 forward，不影响 ae_inference/lm_inference\n",
    "        \"use_lm_loss\": True,\n",
    "    }\n",
    "    return task_config\n",
    "\n",
    "\n",
    "def load_compress_model(\n",
    "    model_id,\n",
    "    adapter_path,\n",
    "    chunk_size=500,\n",
    "    mem_size=1,\n",
    "    compress_ratio=500,\n",
    "    rank=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    加载：base Llama + 压缩 + LoRA + adapter\n",
    "    \"\"\"\n",
    "    task_config = build_task_config(chunk_size, mem_size, compress_ratio)\n",
    "\n",
    "    print(f\"Loading model [{model_id}] with adapter [{adapter_path}] ...\")\n",
    "    model = load_model_with_adapter(\n",
    "        model_id=model_id,\n",
    "        task_config=task_config,\n",
    "        rank=rank,\n",
    "        save_path_and_name=adapter_path,\n",
    "        log=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    tokenizer = model.tokenizer  # modeling 里已经从同一个 model_id 加载过\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def ae_reconstruct(model, tokenizer, context_text, max_new_tokens=512):\n",
    "    \"\"\"\n",
    "    用 ae_inference 复原长上下文：\n",
    "    - 输入：原始长文本 context_text\n",
    "    - 输出：模型重建出来的文本\n",
    "    \"\"\"\n",
    "    # 和 pre_pretrain_data 里保持一致：不加 special tokens，手动加 BOS\n",
    "    context_ids = tokenizer(context_text, add_special_tokens=False)[\"input_ids\"]\n",
    "    if tokenizer.bos_token_id is not None:\n",
    "        context_ids = [tokenizer.bos_token_id] + context_ids\n",
    "\n",
    "    input_ids = torch.LongTensor(context_ids).unsqueeze(0).to(model.device)\n",
    "\n",
    "    inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.ae_inference(inputs)\n",
    "\n",
    "    # ae_inference 返回的是一个 list[int]，直接 decode\n",
    "    recon_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    return recon_text\n",
    "\n",
    "\n",
    "def qa_inference(model, tokenizer, context_text, question, max_new_tokens=256):\n",
    "    \"\"\"\n",
    "    用 lm_inference 做 QA：\n",
    "    - context 走 encoder + mem\n",
    "    - question 作为 decoder 的开头 prompt，模型生成 answer\n",
    "    模板和 instruction_prepare_data.py 里保持一致：\n",
    "      \"### Context:\\\\n\" + context\n",
    "      \"\\\\n### Question:\\\\n\" + question + \"\\\\n### Answer:\\\\n\"\n",
    "    \"\"\"\n",
    "    # context 部分： [BOS] + \"### Context:\\n\" + context\n",
    "    context_ids = (\n",
    "        [tokenizer.bos_token_id]\n",
    "        + tokenizer(\"### Context:\\n\", add_special_tokens=False)[\"input_ids\"]\n",
    "        + tokenizer(context_text, add_special_tokens=False)[\"input_ids\"]\n",
    "    )\n",
    "\n",
    "    # question prompt： \"\\n### Question:\\n\" + question + \"\\n### Answer:\\n\"\n",
    "    question_ids = (\n",
    "        tokenizer(\"\\n### Question:\\n\", add_special_tokens=False)[\"input_ids\"]\n",
    "        + tokenizer(question, add_special_tokens=False)[\"input_ids\"]\n",
    "        + tokenizer(\"\\n### Answer:\\n\", add_special_tokens=False)[\"input_ids\"]\n",
    "    )\n",
    "\n",
    "    input_ids = torch.LongTensor(context_ids).unsqueeze(0).to(model.device)\n",
    "    lm_targets = torch.LongTensor(question_ids).unsqueeze(0).to(model.device)\n",
    "\n",
    "    inputs = {\n",
    "        \"input_ids\": input_ids,      # 长上下文\n",
    "        \"lm_targets\": lm_targets,    # 问题 prompt（不包含答案）\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.lm_inference(inputs, generate_num=max_new_tokens)\n",
    "\n",
    "    answer_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    return answer_text\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ======= 1. 路径配置，按你的实际环境改 =======\n",
    "    model_id = \"/home/syt/project/Cram/model/model_scope_model/LLM-Research/Llama-3.2-1B-Instruct\"\n",
    "    work_dir = \"/home/syt/project/compressor_500/new_code/experiment/llama32_1b_500to1\"\n",
    "    # 如果只做了预训练，用 adapter.pt；做了 SFT，就用 instruction_adapter.pt\n",
    "    adapter_path = os.path.join(work_dir, \"output\", \"instruction_adapter.pt\")\n",
    "\n",
    "    # ======= 2. 加载模型 =======\n",
    "    model, tokenizer = load_compress_model(\n",
    "        model_id=model_id,\n",
    "        adapter_path=adapter_path,\n",
    "        chunk_size=500,\n",
    "        mem_size=1,\n",
    "        compress_ratio=500,\n",
    "        rank=0,  # 用 cuda:0\n",
    "    )\n",
    "\n",
    "    # ======= 3. 示例：AE 复原 =======\n",
    "    long_context = (\n",
    "        \"ICAE (Ge et al., 2024) is an autoencoder framework to compress long contexts into short compact memory slots. The method operates by concatenating designated memory tokens to the end of the input sequence before an encoder processes the entire combined sequence. Subsequently, a decoder reconstructs the original sequence using only the information contained within the memory tokens. ICAE is trained in two main phases. It is first pretrained on massive text data using a combination of autoencoding and language modeling objectives, enabling it to generate memory slots that represent the original context. Following pretraining, the model is fine-tuned on instruction data for the purpose of producing desirable responses to various prompts. An overview of the ICAE framework is shown in Figure 3.\"\n",
    "    )\n",
    "    recon_text = ae_reconstruct(model, tokenizer, long_context)\n",
    "    print(\"=== AE Reconstruction ===\")\n",
    "    print(recon_text)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # ======= 4. 示例：QA 推理 =======\n",
    "    context_text = (\n",
    "        \"ICAE (Ge et al., 2024) is an autoencoder framework to compress long contexts into short compact memory slots. The method operates by concatenating designated memory tokens to the end of the input sequence before an encoder processes the entire combined sequence. Subsequently, a decoder reconstructs the original sequence using only the information contained within the memory tokens. ICAE is trained in two main phases. It is first pretrained on massive text data using a combination of autoencoding and language modeling objectives, enabling it to generate memory slots that represent the original context. Following pretraining, the model is fine-tuned on instruction data for the purpose of producing desirable responses to various prompts. An overview of the ICAE framework is shown in Figure 3.\"\n",
    "    )\n",
    "    question = \"what is ICAE?\"\n",
    "    answer = qa_inference(model, tokenizer, context_text, question, max_new_tokens=128)\n",
    "    print(\"=== QA Answer ===\")\n",
    "    print(answer)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syt/miniconda3/envs/UPL/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pretrain.modeling'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 7\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m AutoTokenizer\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# 如果 modeling.py 在 new_code/pretrain 目录下，这个脚本就放在同一层的 new_code 里，\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# 然后这样 import：\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpretrain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodeling\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m load_model_with_adapter\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mbuild_task_config\u001B[39m(chunk_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m, mem_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, compress_ratio\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m):\n\u001B[1;32m     11\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124;03m    这里的配置要和你训练时保持一致：\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124;03m    chunk_size=500, mem_size=1, compress_ratio=500\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pretrain.modeling'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9e3bab8143e03a2d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
